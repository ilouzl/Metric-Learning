{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Grayscale\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, img_shape=(100,100)):\n",
    "        super(SiameseNet,self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            self.cnn_block(1,4),\n",
    "            self.cnn_block(4,8),\n",
    "            self.cnn_block(8,8),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(img_shape[0]*img_shape[1]*8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,30)\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def cnn_block(self, in_channels, out_channels):\n",
    "        cnn = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels,out_channels,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        return cnn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATTFaces(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.img_ds = ImageFolder(path, transform=transform)\n",
    "        self.n_classes = len(self.img_ds.classes)\n",
    "        self.class_size = 10\n",
    "    \n",
    "    def _get_rand_img_from_class(self, c):\n",
    "        n = np.random.randint(self.class_size)\n",
    "        return self.img_ds[(c*self.class_size + n)]\n",
    "    \n",
    "    def _get_image_pair_same_class(self):\n",
    "        c = np.random.randint(self.n_classes)\n",
    "        return self._get_rand_img_from_class(c), self._get_rand_img_from_class(c)\n",
    "    \n",
    "    def _get_image_pair_different_class(self):\n",
    "        c = np.random.randint(0,self.n_classes,2)\n",
    "        return self._get_rand_img_from_class(c[0]), self._get_rand_img_from_class(c[1])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if np.random.randint(2):\n",
    "            img1, img2 = self._get_image_pair_same_class()\n",
    "        else:\n",
    "            img1, img2 = self._get_image_pair_different_class()\n",
    "            \n",
    "        return img1[0], img2[0], int(img1[1]!=img2[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrasiveLoss(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super(ContrasiveLoss, self).__init__()\n",
    "        self.m = m\n",
    "    \n",
    "    def forward(self, v1, v2, t):\n",
    "        d = torch.norm(v1 - v2, dim=1)\n",
    "        l = (1-t)*d**2 + t*(torch.clamp(self.m - d, min=0.0))**2\n",
    "        return l/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"att_faces\"\n",
    "train_path = os.path.join(dataset_path, 'train_set')\n",
    "test_path = os.path.join(dataset_path, 'test_set')\n",
    "\n",
    "img_shape = (100,100)\n",
    "transforms = Compose([Resize(img_shape), Grayscale(), ToTensor()])\n",
    "ds = ATTFaces(train_path, transform=transforms)\n",
    "dl = DataLoader(ds, batch_size=1)\n",
    "\n",
    "ds_test = ATTFaces(test_path, transform=transforms)\n",
    "dl_test = DataLoader(ds_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6538769340747845\n",
      "0.8168447386473418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liron/miniconda3/envs/camera/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SiameseNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model\n",
      "1.3049782137788133\n",
      "2.357071481309831\n",
      "0.42988966459389\n",
      "3.1383849369734524\n",
      "0.30021020355596495\n",
      "4.580989760950033\n",
      "0.2514655553952919\n",
      "1.7469185554639262\n",
      "0.15701399960700654\n",
      "8.177664301814511\n",
      "0.13493491409656902\n",
      "2.0973949324349817\n",
      "0.09176688249669193\n",
      "1.1415909814334009\n",
      "0.08928141862241318\n",
      "4.235594155360014\n",
      "0.07019017191779009\n",
      "5.170724723068997\n",
      "0.0655686297580299\n",
      "0.8931510563008487\n",
      "0.05878621233554857\n",
      "6.79754934495315\n",
      "0.06188855538833498\n",
      "1.93828111034533\n",
      "0.053964075927421314\n",
      "4.512365977652371\n",
      "0.06514688304244079\n",
      "1.9339282459579408\n",
      "0.04921332340842734\n",
      "3.994299346776679\n",
      "0.05630800061218906\n",
      "1.105116596070584\n",
      "0.0528322378298617\n",
      "0.8722413994371891\n",
      "0.04323187756118083\n",
      "1.0478554924903438\n",
      "0.049786990485251106\n",
      "2.837320207497105\n",
      "0.07088253055541675\n",
      "1.5101246199756861\n",
      "0.04633096406488524\n",
      "2.0505081094428896\n",
      "0.04993934898986481\n",
      "1.6841161077003926\n",
      "0.03212068009384287\n",
      "2.794539422355592\n",
      "0.03700368360602928\n",
      "2.836395698511042\n",
      "0.03314233483358597\n",
      "1.0497476268394894\n",
      "0.02168465073117356\n",
      "2.0418017124664036\n",
      "0.03648971321195707\n",
      "6.770179844261147\n",
      "0.029942914314257604\n",
      "0.8202055930229836\n",
      "0.04094486097145515\n",
      "1.1142145304856967\n",
      "0.04689064522768604\n",
      "2.448292381602223\n",
      "0.03436645498421664\n",
      "2.919643192682415\n",
      "0.023954313580082574\n",
      "3.5489554109377788\n",
      "0.025294685316014996\n",
      "0.9044977042917162\n",
      "0.02043930724139282\n",
      "0.7261281565879472\n",
      "saved model\n",
      "0.01376195028113822\n",
      "3.8416538220643996\n",
      "0.02615401505020903\n",
      "0.8905871876282617\n",
      "0.03021413387394811\n",
      "1.2926818550005554\n",
      "0.01985088542553967\n",
      "0.6654343401482401\n",
      "saved model\n",
      "0.03563879520836053\n",
      "0.5710783910565078\n",
      "saved model\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNet(img_shape)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = ContrasiveLoss(2.)\n",
    "\n",
    "EPOCHS = 40\n",
    "min_loss = 100000\n",
    "for n in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, mb in enumerate(dl,0):\n",
    "        img1, img2, label = mb[0].to(device), mb[1].to(device), mb[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        emb1 = model(img1)\n",
    "        emb2 = model(img2)\n",
    "        loss = loss_fn(emb1, emb2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(ds)\n",
    "    print(total_loss)\n",
    "#     if total_loss < min_loss:\n",
    "#         min_loss = total_loss\n",
    "#         torch.save(model, \"model.checkpoint\")\n",
    "#         print(\"saved model\")\n",
    "        \n",
    "    model.eval()\n",
    "    total_loss_test = 0\n",
    "    for i, mb in enumerate(dl_test,0):\n",
    "        img1, img2, label = mb[0].to(device), mb[1].to(device), mb[2].to(device)\n",
    "        emb1 = model(img1)\n",
    "        emb2 = model(img2)\n",
    "        loss = loss_fn(emb1, emb2, label)\n",
    "        total_loss_test += loss.item()\n",
    "    total_loss_test /= len(ds_test)\n",
    "    print(total_loss_test)\n",
    "    if total_loss_test < min_loss:\n",
    "        min_loss = total_loss_test\n",
    "        torch.save(model, \"model.checkpoint\")\n",
    "        print(\"saved model\")\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1910221576690674 tensor([0], device='cuda:0')\n",
      "0.7320285439491272 tensor([1], device='cuda:0')\n",
      "5.896364212036133 tensor([1], device='cuda:0')\n",
      "2.2141172885894775 tensor([0], device='cuda:0')\n",
      "3.546520233154297 tensor([0], device='cuda:0')\n",
      "2.2753489017486572 tensor([0], device='cuda:0')\n",
      "1.053887128829956 tensor([1], device='cuda:0')\n",
      "7.313989162445068 tensor([1], device='cuda:0')\n",
      "1.371145248413086 tensor([1], device='cuda:0')\n",
      "7.882805347442627 tensor([1], device='cuda:0')\n",
      "1.5637966394424438 tensor([1], device='cuda:0')\n",
      "7.834571838378906 tensor([1], device='cuda:0')\n",
      "6.106356620788574 tensor([0], device='cuda:0')\n",
      "6.606673717498779 tensor([1], device='cuda:0')\n",
      "6.308690071105957 tensor([1], device='cuda:0')\n",
      "5.355000019073486 tensor([1], device='cuda:0')\n",
      "0.8447266817092896 tensor([0], device='cuda:0')\n",
      "3.3678133487701416 tensor([1], device='cuda:0')\n",
      "6.8026604652404785 tensor([1], device='cuda:0')\n",
      "2.392662763595581 tensor([1], device='cuda:0')\n",
      "4.4945244789123535 tensor([1], device='cuda:0')\n",
      "0.7540794610977173 tensor([0], device='cuda:0')\n",
      "0.3683965802192688 tensor([0], device='cuda:0')\n",
      "3.5889155864715576 tensor([1], device='cuda:0')\n",
      "7.647062301635742 tensor([1], device='cuda:0')\n",
      "1.334554672241211 tensor([0], device='cuda:0')\n",
      "2.6203219890594482 tensor([0], device='cuda:0')\n",
      "3.1679575443267822 tensor([0], device='cuda:0')\n",
      "3.9921953678131104 tensor([1], device='cuda:0')\n",
      "0.7777377963066101 tensor([1], device='cuda:0')\n",
      "0.7367878556251526 tensor([0], device='cuda:0')\n",
      "7.31484317779541 tensor([1], device='cuda:0')\n",
      "0.804372251033783 tensor([0], device='cuda:0')\n",
      "6.208764553070068 tensor([1], device='cuda:0')\n",
      "0.4921955168247223 tensor([0], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "5.38262939453125 tensor([1], device='cuda:0')\n",
      "7.027657985687256 tensor([1], device='cuda:0')\n",
      "4.608881950378418 tensor([1], device='cuda:0')\n",
      "3.178164482116699 tensor([1], device='cuda:0')\n",
      "1.0030691623687744 tensor([0], device='cuda:0')\n",
      "2.8618555068969727 tensor([0], device='cuda:0')\n",
      "5.166162014007568 tensor([1], device='cuda:0')\n",
      "5.602086544036865 tensor([1], device='cuda:0')\n",
      "1.991493821144104 tensor([1], device='cuda:0')\n",
      "7.280367374420166 tensor([0], device='cuda:0')\n",
      "1.2471287250518799 tensor([0], device='cuda:0')\n",
      "0.5562224984169006 tensor([0], device='cuda:0')\n",
      "1.3299407958984375 tensor([1], device='cuda:0')\n",
      "3.917633295059204 tensor([1], device='cuda:0')\n",
      "2.7371602058410645 tensor([1], device='cuda:0')\n",
      "4.949893474578857 tensor([1], device='cuda:0')\n",
      "3.965853691101074 tensor([1], device='cuda:0')\n",
      "1.9113966226577759 tensor([0], device='cuda:0')\n",
      "0.5571004748344421 tensor([0], device='cuda:0')\n",
      "8.434943199157715 tensor([1], device='cuda:0')\n",
      "0.8080145120620728 tensor([0], device='cuda:0')\n",
      "1.668890357017517 tensor([1], device='cuda:0')\n",
      "1.3386108875274658 tensor([1], device='cuda:0')\n",
      "3.6878199577331543 tensor([1], device='cuda:0')\n",
      "1.0973707437515259 tensor([0], device='cuda:0')\n",
      "0.9254242777824402 tensor([0], device='cuda:0')\n",
      "2.293297529220581 tensor([1], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "3.2892026901245117 tensor([1], device='cuda:0')\n",
      "6.73530387878418 tensor([0], device='cuda:0')\n",
      "3.9049041271209717 tensor([1], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "2.795478343963623 tensor([0], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "5.105064868927002 tensor([1], device='cuda:0')\n",
      "4.514154434204102 tensor([0], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "7.520297050476074 tensor([0], device='cuda:0')\n",
      "0.2645367681980133 tensor([0], device='cuda:0')\n",
      "5.854544162750244 tensor([1], device='cuda:0')\n",
      "3.1679575443267822 tensor([0], device='cuda:0')\n",
      "6.512786388397217 tensor([0], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "3.416156768798828 tensor([0], device='cuda:0')\n",
      "0.6807066202163696 tensor([1], device='cuda:0')\n",
      "0.0 tensor([0], device='cuda:0')\n",
      "0.5058457255363464 tensor([0], device='cuda:0')\n",
      "1.3877732753753662 tensor([0], device='cuda:0')\n",
      "7.379981517791748 tensor([1], device='cuda:0')\n",
      "1.3013755083084106 tensor([0], device='cuda:0')\n",
      "3.31217098236084 tensor([0], device='cuda:0')\n",
      "1.321349024772644 tensor([0], device='cuda:0')\n",
      "7.222630977630615 tensor([0], device='cuda:0')\n",
      "1.9820988178253174 tensor([1], device='cuda:0')\n",
      "1.612040400505066 tensor([1], device='cuda:0')\n",
      "2.960448741912842 tensor([1], device='cuda:0')\n",
      "0.6635439991950989 tensor([0], device='cuda:0')\n",
      "1.9868382215499878 tensor([1], device='cuda:0')\n",
      "1.3013755083084106 tensor([0], device='cuda:0')\n",
      "2.6646177768707275 tensor([1], device='cuda:0')\n",
      "1.4160797595977783 tensor([0], device='cuda:0')\n",
      "1.1265321969985962 tensor([0], device='cuda:0')\n",
      "0.3670003116130829 tensor([0], device='cuda:0')\n",
      "1.2543153762817383 tensor([0], device='cuda:0')\n",
      "0.03563879520836053\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"model.checkpoint\")\n",
    "model.eval()\n",
    "\n",
    "for i, mb in enumerate(dl_test,0):\n",
    "    img1, img2, label = mb[0].to(device), mb[1].to(device), mb[2].to(device)\n",
    "    emb1 = model(img1)\n",
    "    emb2 = model(img2)\n",
    "    dist = torch.norm(emb1 - emb2, dim=1)\n",
    "    print(dist.item(), label)\n",
    "print(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
